{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCjp0qisnlbEkfbVFo/rCo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drsowmyajagadeesan/webscrap/blob/main/job_scrap2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "R-qxtdbEsduD",
        "outputId": "63107071-9c02-4e78-e71b-a11ae1ea062c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-073bdf04be55>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mui\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import urllib\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "def find_jobs_from(website, job_title, location, desired_characs, filename=\"results.xls\"):\n",
        "    \"\"\"\n",
        "    This function extracts all the desired characteristics of all new job postings\n",
        "    of the title and location specified and returns them in single file.\n",
        "    The arguments it takes are:\n",
        "        - Website: to specify which website to search (options: 'Indeed' or 'CWjobs')\n",
        "        - Job_title\n",
        "        - Location\n",
        "        - Desired_characs: this is a list of the job characteristics of interest,\n",
        "            from titles, companies, links and date_listed.\n",
        "        - Filename: to specify the filename and format of the output.\n",
        "            Default is .xls file called 'results.xls'\n",
        "    \"\"\"\n",
        "\n",
        "    if website == 'Indeed':\n",
        "        job_soup = load_indeed_jobs_div(job_title, location)\n",
        "        jobs_list, num_listings = extract_job_information_indeed(job_soup, desired_characs)\n",
        "\n",
        "    if website == 'CWjobs':\n",
        "        location_of_driver = os.getcwd()\n",
        "        driver = initiate_driver(location_of_driver, browser='chrome')\n",
        "        job_soup = make_job_search(job_title, location, driver)\n",
        "        jobs_list, num_listings = extract_job_information_cwjobs(job_soup, desired_characs)\n",
        "\n",
        "    save_jobs_to_excel(jobs_list, filename)\n",
        "\n",
        "    print('{} new job postings retrieved from {}. Stored in {}.'.format(num_listings,\n",
        "                                                                          website, filename))\n",
        "\n",
        "\n",
        "## ======================= GENERIC FUNCTIONS ======================= ##\n",
        "\n",
        "def save_jobs_to_excel(jobs_list, filename):\n",
        "    jobs = pd.DataFrame(jobs_list)\n",
        "    jobs.to_excel(filename)\n",
        "\n",
        "\n",
        "\n",
        "## ================== FUNCTIONS FOR INDEED.CO.UK =================== ##\n",
        "\n",
        "def load_indeed_jobs_div(job_title, location):\n",
        "    getVars = {'q' : job_title, 'l' : location, 'fromage' : 'last', 'sort' : 'date'}\n",
        "    url = ('https://www.indeed.co.uk/jobs?' + urllib.parse.urlencode(getVars))\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    job_soup = soup.find(id=\"resultsCol\")\n",
        "    return job_soup\n",
        "\n",
        "def extract_job_information_indeed(job_soup, desired_characs):\n",
        "    job_elems = job_soup.find_all('div', class_='jobsearch-SerpJobCard')\n",
        "\n",
        "    cols = []\n",
        "    extracted_info = []\n",
        "\n",
        "\n",
        "    if 'titles' in desired_characs:\n",
        "        titles = []\n",
        "        cols.append('titles')\n",
        "        for job_elem in job_elems:\n",
        "            titles.append(extract_job_title_indeed(job_elem))\n",
        "        extracted_info.append(titles)\n",
        "\n",
        "    if 'companies' in desired_characs:\n",
        "        companies = []\n",
        "        cols.append('companies')\n",
        "        for job_elem in job_elems:\n",
        "            companies.append(extract_company_indeed(job_elem))\n",
        "        extracted_info.append(companies)\n",
        "\n",
        "    if 'links' in desired_characs:\n",
        "        links = []\n",
        "        cols.append('links')\n",
        "        for job_elem in job_elems:\n",
        "            links.append(extract_link_indeed(job_elem))\n",
        "        extracted_info.append(links)\n",
        "\n",
        "    if 'date_listed' in desired_characs:\n",
        "        dates = []\n",
        "        cols.append('date_listed')\n",
        "        for job_elem in job_elems:\n",
        "            dates.append(extract_date_indeed(job_elem))\n",
        "        extracted_info.append(dates)\n",
        "\n",
        "    jobs_list = {}\n",
        "\n",
        "    for j in range(len(cols)):\n",
        "        jobs_list[cols[j]] = extracted_info[j]\n",
        "\n",
        "    num_listings = len(extracted_info[0])\n",
        "\n",
        "    return jobs_list, num_listings\n",
        "\n",
        "\n",
        "def extract_job_title_indeed(job_elem):\n",
        "    title_elem = job_elem.find('h2', class_='title')\n",
        "    title = title_elem.text.strip()\n",
        "    return title\n",
        "\n",
        "def extract_company_indeed(job_elem):\n",
        "    company_elem = job_elem.find('span', class_='company')\n",
        "    company = company_elem.text.strip()\n",
        "    return company\n",
        "\n",
        "def extract_link_indeed(job_elem):\n",
        "    link = job_elem.find('a')['href']\n",
        "    link = 'www.Indeed.co.uk/' + link\n",
        "    return link\n",
        "\n",
        "def extract_date_indeed(job_elem):\n",
        "    date_elem = job_elem.find('span', class_='date')\n",
        "    date = date_elem.text.strip()\n",
        "    return date\n",
        "\n",
        "\n",
        "\n",
        "## ================== FUNCTIONS FOR CWJOBS.CO.UK =================== ##\n",
        "\n",
        "\n",
        "def initiate_driver(location_of_driver, browser):\n",
        "    if browser == 'chrome':\n",
        "        driver = webdriver.Chrome(executable_path=(location_of_driver + \"/chromedriver\"))\n",
        "    elif browser == 'firefox':\n",
        "        driver = webdriver.Firefox(executable_path=(location_of_driver + \"/firefoxdriver\"))\n",
        "    elif browser == 'safari':\n",
        "        driver = webdriver.Safari(executable_path=(location_of_driver + \"/safaridriver\"))\n",
        "    elif browser == 'edge':\n",
        "        driver = webdriver.Edge(executable_path=(location_of_driver + \"/edgedriver\"))\n",
        "    return driver\n",
        "\n",
        "def make_job_search(job_title, location, driver):\n",
        "    driver.get('https://www.cwjobs.co.uk/')\n",
        "\n",
        "    # Select the job box\n",
        "    job_title_box = driver.find_element_by_name('Keywords')\n",
        "\n",
        "    # Send job information\n",
        "    job_title_box.send_keys(job_title)\n",
        "\n",
        "    # Selection location box\n",
        "    location_box = driver.find_element_by_id('location')\n",
        "\n",
        "    # Send location information\n",
        "    location_box.send_keys(location)\n",
        "\n",
        "    # Find Search button\n",
        "    search_button = driver.find_element_by_id('search-button')\n",
        "    search_button.click()\n",
        "\n",
        "    driver.implicitly_wait(5)\n",
        "\n",
        "    page_source = driver.page_source\n",
        "\n",
        "    job_soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "\n",
        "    return job_soup\n",
        "\n",
        "\n",
        "def extract_job_information_cwjobs(job_soup, desired_characs):\n",
        "\n",
        "    job_elems = job_soup.find_all('div', class_=\"job\")\n",
        "\n",
        "    cols = []\n",
        "    extracted_info = []\n",
        "\n",
        "    if 'titles' in desired_characs:\n",
        "        titles = []\n",
        "        cols.append('titles')\n",
        "        for job_elem in job_elems:\n",
        "            titles.append(extract_job_title_cwjobs(job_elem))\n",
        "        extracted_info.append(titles)\n",
        "\n",
        "\n",
        "    if 'companies' in desired_characs:\n",
        "        companies = []\n",
        "        cols.append('companies')\n",
        "        for job_elem in job_elems:\n",
        "            companies.append(extract_company_cwjobs(job_elem))\n",
        "        extracted_info.append(companies)\n",
        "\n",
        "    if 'links' in desired_characs:\n",
        "        links = []\n",
        "        cols.append('links')\n",
        "        for job_elem in job_elems:\n",
        "            links.append(extract_link_cwjobs(job_elem))\n",
        "        extracted_info.append(links)\n",
        "\n",
        "    if 'date_listed' in desired_characs:\n",
        "        dates = []\n",
        "        cols.append('date_listed')\n",
        "        for job_elem in job_elems:\n",
        "            dates.append(extract_date_cwjobs(job_elem))\n",
        "        extracted_info.append(dates)\n",
        "\n",
        "    jobs_list = {}\n",
        "\n",
        "    for j in range(len(cols)):\n",
        "        jobs_list[cols[j]] = extracted_info[j]\n",
        "\n",
        "    num_listings = len(extracted_info[0])\n",
        "\n",
        "    return jobs_list, num_listings\n",
        "\n",
        "\n",
        "def extract_job_title_cwjobs(job_elem):\n",
        "    title_elem = job_elem.find('h2')\n",
        "    title = title_elem.text.strip()\n",
        "    return title\n",
        "\n",
        "def extract_company_cwjobs(job_elem):\n",
        "    company_elem = job_elem.find('h3')\n",
        "    company = company_elem.text.strip()\n",
        "    return company\n",
        "\n",
        "def extract_link_cwjobs(job_elem):\n",
        "    link = job_elem.find('a')['href']\n",
        "    return link\n",
        "\n",
        "def extract_date_cwjobs(job_elem):\n",
        "    link_elem = job_elem.find('li', class_='date-posted')\n",
        "    link = link_elem.text.strip()\n",
        "    return link\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P96gu71Yutx2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}