{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqhp+FGhJN3U+X47Zs4aDz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drsowmyajagadeesan/webscrap/blob/main/Indeed_Job.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QyS2tIHnpjlj"
      },
      "outputs": [],
      "source": [
        "def load_indeed_jobs_div(job_title, location):\n",
        "    getVars = {'q' : job_title, 'l' : location, 'fromage' : 'last', 'sort' : 'date'}\n",
        "    url = ('https://www.indeed.co.in/jobs?' + urllib.parse.urlencode(getVars))\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    job_soup = soup.find(id=\"resultsCol\")\n",
        "    return job_soup\n",
        "\n",
        "#2. Extracting job details : job titles, companies, link to the full job profile, date listed\n",
        "def extract_job_title_indeed(job_elem):\n",
        "    title_elem = job_elem.find('h2', class_='title')\n",
        "    title = title_elem.text.strip()\n",
        "    return title\n",
        "\n",
        "def extract_company_indeed(job_elem):\n",
        "    company_elem = job_elem.find('span', class_='company')\n",
        "    company = company_elem.text.strip()\n",
        "    return company\n",
        "\n",
        "def extract_link_indeed(job_elem):\n",
        "    link = job_elem.find('a')['href']\n",
        "    link = 'www.indeed.co.in/' + link\n",
        "    return link\n",
        "\n",
        "def extract_date_indeed(job_elem):\n",
        "    date_elem = job_elem.find('span', class_='date')\n",
        "    date = date_elem.text.strip()\n",
        "    return date\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Iterating over each job listing\n",
        "job_elems = job_soup.find_all('div', class_='jobsearch-SerpJobCard')\n",
        "\n",
        "#3a looped over and added them to a list\n",
        "\n",
        "cols = []\n",
        "extracted_info = []\n",
        "\n",
        "\n",
        "if 'titles' in desired_characs:\n",
        "    titles = []\n",
        "    cols.append('titles')\n",
        "    for job_elem in job_elems:\n",
        "        titles.append(extract_job_title_indeed(job_elem))\n",
        "    extracted_info.append(titles)\n",
        "\n",
        "if 'companies' in desired_characs:\n",
        "    companies = []\n",
        "    cols.append('companies')\n",
        "    for job_elem in job_elems:\n",
        "        companies.append(extract_company_indeed(job_elem))\n",
        "    extracted_info.append(companies)\n",
        "\n",
        "if 'links' in desired_characs:\n",
        "    links = []\n",
        "    cols.append('links')\n",
        "    for job_elem in job_elems:\n",
        "        links.append(extract_link_indeed(job_elem))\n",
        "    extracted_info.append(links)\n",
        "\n",
        "if 'date_listed' in desired_characs:\n",
        "    dates = []\n",
        "    cols.append('date_listed')\n",
        "    for job_elem in job_elems:\n",
        "        dates.append(extract_date_indeed(job_elem))\n",
        "    extracted_info.append(dates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "AVhpb27HvAZP",
        "outputId": "fc14d434-0829-4580-ddb7-2f1b70b22ccc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-64f999077843>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#3 Iterating over each job listing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjob_elems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_soup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jobsearch-SerpJobCard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#3a looped over and added them to a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'job_soup' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finally, I brought all of these into a jobs_list, which could then be exported to the chosen format â€” an Excel, DataFrame or otherwise:\n",
        "\n",
        "jobs_list = {}\n",
        "\n",
        "for j in range(len(cols)):\n",
        "    jobs_list[cols[j]] = extracted_info[j]\n",
        "\n",
        "num_listings = len(extracted_info[0])"
      ],
      "metadata": {
        "id": "j5kPG3ouu6b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the jobs\n",
        "def save_jobs_to_excel(jobs_list, filename):\n",
        "    jobs = pd.DataFrame(jobs_list)\n",
        "    jobs.to_excel(filename)"
      ],
      "metadata": {
        "id": "t0AwAkqe3oHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Integrating into a single function call\n",
        "def find_jobs_from(website, job_title, location, desired_characs, filename=\"results.xls\"):\n",
        "    \"\"\"\n",
        "    This function extracts all the desired characteristics of all new job postings\n",
        "    of the title and location specified and returns them in single file.\n",
        "    The arguments it takes are:\n",
        "        - Website: to specify which website to search (options: 'Indeed' or 'CWjobs')\n",
        "        - Job_title\n",
        "        - Location\n",
        "        - Desired_characs: this is a list of the job characteristics of interest,\n",
        "            from titles, companies, links and date_listed.\n",
        "        - Filename: to specify the filename and format of the output.\n",
        "            Default is .xls file called 'results.xls'\n",
        "    \"\"\"\n",
        "\n",
        "    if website == 'Indeed':\n",
        "        job_soup = load_indeed_jobs_div(job_title, location)\n",
        "        jobs_list, num_listings = extract_job_information_indeed(job_soup, desired_characs)\n",
        "\n",
        "    if website == 'CWjobs':\n",
        "        # TO DO LATER\n",
        "\n",
        "        save_jobs_to_excel(jobs_list, filename)\n",
        "\n",
        "def extract_job_information_indeed(job_soup, desired_characs):\n",
        "    job_elems = job_soup.find_all('div', class_='jobsearch-SerpJobCard')\n",
        "\n",
        "    cols = []\n",
        "    extracted_info = []\n",
        "\n",
        "\n",
        "    if 'titles' in desired_characs:\n",
        "        titles = []\n",
        "        cols.append('titles')\n",
        "        for job_elem in job_elems:\n",
        "            titles.append(extract_job_title_indeed(job_elem))\n",
        "        extracted_info.append(titles)\n",
        "\n",
        "    if 'companies' in desired_characs:\n",
        "        companies = []\n",
        "        cols.append('companies')\n",
        "        for job_elem in job_elems:\n",
        "            companies.append(extract_company_indeed(job_elem))\n",
        "        extracted_info.append(companies)\n",
        "\n",
        "    if 'links' in desired_characs:\n",
        "        links = []\n",
        "        cols.append('links')\n",
        "        for job_elem in job_elems:\n",
        "            links.append(extract_link_indeed(job_elem))\n",
        "        extracted_info.append(links)\n",
        "\n",
        "    if 'date_listed' in desired_characs:\n",
        "        dates = []\n",
        "        cols.append('date_listed')\n",
        "        for job_elem in job_elems:\n",
        "            dates.append(extract_date_indeed(job_elem))\n",
        "        extracted_info.append(dates)\n",
        "\n",
        "    jobs_list = {}\n",
        "\n",
        "    for j in range(len(cols)):\n",
        "        jobs_list[cols[j]] = extracted_info[j]\n",
        "\n",
        "    num_listings = len(extracted_info[0])\n",
        "\n",
        "    return jobs_list, num_listings\n",
        "    print('{} new job postings retrieved. Stored in {}.'.format(num_listings, filename))"
      ],
      "metadata": {
        "id": "rG1vdPeP3wKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2rCGMwpv39GA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}